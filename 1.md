
## 运维的日子


我之前不是做运维的, 在逃离上海的时候意外做了数据库运维, 一做做了两年半. 先前以为是挺无聊的事情, 却意外地遇到了很多有趣的事情, 想来必须要做个总结, 不然之后很容易就忘了. 总体上说, 遇到的这些事, 对"二线及以下"互联网公司, 应该都挺实用的. 


### 上云

现在的故事多是发生在云上, 作为一个大公司, 没有私有云那是说不过去的. 我去做运维的时候, 正好碰上数据库上云这事. 

开头我遇到的是Couchbase, 一个内存数据库, 可以理解为memcache的集群版. 用内存的系统, 套了个虚机, 以我体系结构实验室的知识, 理应一切正常, 然而一上来就遇到下马威. 我们当时要搭建一个1T内存的Couchbase集群, 用4台3X0G内存的服务器, 每个服务器上开3个100G内存的虚机--请不要问我为什么要这样起虚机, 反正就是要--于是开起来了. 很顺利的装上Couchbase, 搭建好集群, 交给业务使用. 

<插图: 4个机器虚成12个虚机再组成一个集群>

半个小时之后, 被找上门了: "缓存性能不稳定, 100ms还没拿到数据". 我直接傻眼了. 100ms? 这怎么可能呢. 然后就自己写个程序测试, 的确很慢, 而且很不稳定. 有一些请求是1ms, 有一些慢到150ms. 是不是机器有问题? 于是请IAAS组帮我们看机器正常不正常. 一切都很正常. 于是重装软件, 重启虚机. 别说, 重启之后的确会好一些. 然而也就好一会. 就这么折腾了一阵, 突发奇想, 宿主机网络和虚机网络都测一下. 竟然发现诡异的地方: 宿主机的延迟在0.1ms, 虚机在0.5ms到20ms不等. 那至少确定是虚拟化出了问题. 就继续交给给IAAS组. 当然是没有解答. 

于是就比较尴尬了, 于是很悲催地悬在那里空想怎么办. 东搞搞西搞搞弄了两天, 灵光闪现google了一下vmware latency. 神奇地收获了一份专门讲vmware延迟调优的文档! 心里感叹真是专业的公司啊. 于是就按图索骥, 第一条: BIOS设置. 忽略, 服务器当然都是标配啊. 跳到第二条. 结果都做了一遍, 还是没帮助. 只能回过头来再试图看看BIOS. 不看不知道, 一看吓一跳: **当前处于省电模式**. =.=!!! 线上服务器开着省电模式. 实在是惊喜啊. 改成性能优先, 世界清净了.


PS. 上图这种集群拓扑是很危险的, 引以为戒.

### 隔离

互联网么, 关键就是"要快". 比如说上云这事, 说上就上, 上了再说. 前面说到我是搞缓存服务器, 那自然还有其他的很多服务器啊, 比如说FTP. FTP也上云了, 就放在咱们缓存服务器的虚拟机边上.

<插图, 你好, 我是缓存, 你好, 我是FTP, 我们都是服务器!>

万兆服务器带宽啊, 我都不知道这个问题当时是怎么找出来的了. 我们去找FTP的owner, 说你的机器得迁走. 小伙还说"不行, 这个服务很重要的不能停". 我只记得当时直想飙脏话. 一个FTP服务器还不能停了, 咱A股都能熔断好吗.

幸好和IAAS的哥们铁, 我就开始嚷嚷让他们上隔离. 终于有一天, 哥们说有隔离了, 最大不超过宿主机的30%. 我就盯着他看, 我不说话. 后来么总是说了, 终究也没有在我离开之前看到结果.

我想说的是: 隔离必须是弹性隔离, 在没有竞争的时候, 应该让guest vm尽情的用, 性能最大化; 有竞争的时候, 再做限制性的隔离. 私有云和公有云不一样, 公有云是故意隔离了收费的, 私有云没法收费肯定是提高利用率优先. 实现其实很简单, 写个百八十行的脚本就能搞定吧, 每秒钟check一下各个虚机的带宽, 调整一下配置. 都不用和openstack有关系啊.

### Zabbix

我在做开发的时候, 从来没听说过监控系统(咱们代码写得好啊:). 第一次接触zabbix的时候, 觉得真心难用. 不过总得用, 感谢同事搭了雏形, 还带我入门. 然后发现, 简直太好了啊. 估计有很多人要说我没见识, 还有这个那个更好的. 那都没有用! 我就觉得, 开始**用一个东西, 就要用好它**. 什么叫用好它呢? 就像学英语, 要Think in English; 学Java, 要Think in Java; 用Zabbix的时候, 就要想"怎么用Zabbix来满足当前需求", 而不是"这个需求能不能用Zabbix实现". 

很多公司喜欢自己做监控系统, 我其实比较迷惑, 为什么不把时间放在更有意义的事情上去.

有半年多的时间, 我都是逢人就推荐Zabbix. 做了运维再回头看开发, 真的好多项目, 都是盲操啊. 后面我会说到, 我最有成效的一次推荐. 虽然人家上了nagios, 然而精髓在于"**知道监控的重要性**"嘛, 后来他们监控的详细程度, 让运维都汗颜. 

<插图, 有监控之前: 你们的问题! 不! 是你们的问题! 有监控之后: 你们的问题! 对对, 是我们的问题>

在推荐zabbix的过程中, 我遇到很多人以为它只能监控CPU, 磁盘, 内存, 不能对app server做监控. 不知道他们是怎么了解这一点的, 但是很多好的东西, 就是这样被略过去的. 二手烟可恶, 二手信息有毒, 看书要看原著, 调研要上官网. 

使用Zabbix的时候, 也发生过一件有趣的事. 因为Couchbase是集群系统, 而Zabbix是按server监控的, 有很多报警, 会每台server都报一次. 比如我们有10台server组成集群, 数据是分散存储的, 像数据超出内存这种问题, 10台server都报一遍. 而实际上, 我们只需要知道"这个集群数据超出容量". 那要怎么办呢? Zabbix提供了一种Calculated item, 可以把收集到的数据聚合成一个新的metrics, 用它来报警, 而不是设定在每个Server的metrics上.

开始我也是这么想的, 后来发现维护太复杂. 于是用了个技巧: 我不需要每个机器都报, 也不需要一个聚合项, 只要让这个集群中的一台机器负责集群metrics的报警就可以了啊. 那要怎么来确定谁来报警呢? 我增加了一个fakemaster的metrics, 每台机器按照集群机器列表, 计算一下自己是不是fakemaster. 只有fakemaster负责报警.

然后, 记得要用Screen. 

### 应用对监控的支持

说到监控, 其实还没完, 很重要的一部分是应用怎么去支持监控. 最近看到一些应用的监控, 采用侵入式的框架, 在中间件上实现. 我是不大喜欢啦. 侵入式这个名词就惹人烦. 

我接触到的情况, 现在的开发, 多能用日志. 开发更喜欢"做监控的人"能从日志中提取metrics, 这样他们就什么也不用做. 所以ELK流行是很有道理的. 然而监控和日志绝对不是一回事, 日志记录的是事件, 监控的metrics描述的是状态. 应用和监控系统的接口, 就应该是一个metrics列表.

在折腾tigase的时候, 看到它有个statistics list, 唯一的问题是通过继承实现的, 添加起来不怎么方便. 所以我就改了一个使用注册的.

	interface Statistics {
		List<Pair<String, Object>> getStatistics();
	}
	
	class Office {
		public void register(Statistics s){...}
		public List<Pair<String, Object>> getStatistics(){...}
	}


想法很简单, 需要metrics列表的时候, 调用Offic.getStatistics就可以获取所有注册了的模块的信息. 本来要参考一下logger的实现, 让它看起来更好一点. 不过么, 比没有已经好太多了! 

我觉得, **把metrics留给开发**是很重要的, 就像review code一样, 开发得考虑可能的瓶颈和故障点, 知道自己的代码脆弱的地方.  

### YCSB性能测试

为了能了解不同数据库的性能, 我们开始对Couchbase, MongoDB还有Redis做性能测试, 后面两个比较容易, 但是用YCSB对Couchbase做性能测试的时候, 就有点感觉不大靠谱. 因为YCSB是线程模型, 所以, 很长一段时间里, 我们以为Couchbase的性能差不多就那个样子. 挺长一段时间之后, 为了排查问题在服务端还是客户端, 我写了个异步的benchmark, 才发现Couchbase很有潜力.

因为用Java比较方便, 当时用Java写的客户端, 单线程轻松上30kqps. 我就多找了几台机器用来加压力, 顿感深不见底啊! 于是我搞了个Hadoop帐号, 非要测个结果出来.

当时正好有3台机器空着, 24核N多内存千兆带宽. 因为主要是测软件性能, 所以内存没关系. 经过多轮测试, 服务器还是2.1版本, 用异步调用的情况下, 15个Hadoop线程, 能把3台Couchbase压满, 接近1Mqps的读. 写减半. 是怎么知道压满了呢? 因为15个线程和50个线程的结果是一样的.

当时的Couchbase, 最多能用到4核. 现在应该更厉害了.

经此一役, 我的感受是, 现在的大型互联网服务, 真需要Hadoop来做压力测试, 用来模拟百万用户在线什么的. AB得升级成HadoopAB, 不知道这个有人做了没. 以及, 单纯说每天几十亿的访问, 真不算什么压力.


### 部署拓扑

因为多机房的缘故, 遇到了好几次因为部署拓扑带来的性能和稳定性的问题. 

这是第一个问题: 需要为电信和联通的用户都提供好服务, 就必须在电信和联通机房都部署服务. 所以可能会这样安排服务器:

<deploy1>
或者
<deploy2>

原则上说, 这两种部署, 在**正确地处理数据一致性**的情况下, 都是能工作的.



